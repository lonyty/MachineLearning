{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiclass situation with mutually exclusive classes --> Softmax Function\n",
    "#1 Output note per class (dummy variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cleaned data set\n",
    "df = pd.read_csv('cleaned_data/Cleaned Data.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Revenue Growth</th>\n",
       "      <th>Cost of Revenue</th>\n",
       "      <th>Gross Profit</th>\n",
       "      <th>R&amp;D Expenses</th>\n",
       "      <th>SG&amp;A Expense</th>\n",
       "      <th>Operating Expenses</th>\n",
       "      <th>Operating Income</th>\n",
       "      <th>Interest Expense</th>\n",
       "      <th>Earnings before Tax</th>\n",
       "      <th>...</th>\n",
       "      <th>Consumer Cyclical</th>\n",
       "      <th>Consumer Defensive</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Financial Services</th>\n",
       "      <th>Healthcare</th>\n",
       "      <th>Industrials</th>\n",
       "      <th>Real Estate</th>\n",
       "      <th>Technology</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>Signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.734148e+09</td>\n",
       "      <td>1.1737</td>\n",
       "      <td>2.805625e+09</td>\n",
       "      <td>9.285226e+08</td>\n",
       "      <td>1.083303e+08</td>\n",
       "      <td>3.441414e+08</td>\n",
       "      <td>7.939267e+08</td>\n",
       "      <td>1.345959e+08</td>\n",
       "      <td>1.214869e+07</td>\n",
       "      <td>1.753823e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.790960e+10</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>1.153980e+10</td>\n",
       "      <td>6.369800e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.474300e+09</td>\n",
       "      <td>3.412400e+09</td>\n",
       "      <td>2.957400e+09</td>\n",
       "      <td>3.024000e+08</td>\n",
       "      <td>2.707700e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.727000e+09</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>3.523600e+09</td>\n",
       "      <td>2.203400e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.480500e+09</td>\n",
       "      <td>1.598700e+09</td>\n",
       "      <td>6.047000e+08</td>\n",
       "      <td>6.040000e+07</td>\n",
       "      <td>4.669000e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.972400e+10</td>\n",
       "      <td>0.0083</td>\n",
       "      <td>1.304100e+10</td>\n",
       "      <td>6.683000e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.709000e+09</td>\n",
       "      <td>4.162000e+09</td>\n",
       "      <td>2.521000e+09</td>\n",
       "      <td>2.840000e+08</td>\n",
       "      <td>2.382000e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.268000e+09</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>5.297000e+09</td>\n",
       "      <td>2.971000e+09</td>\n",
       "      <td>1.220000e+08</td>\n",
       "      <td>1.505000e+09</td>\n",
       "      <td>1.704000e+09</td>\n",
       "      <td>1.267000e+09</td>\n",
       "      <td>1.220000e+08</td>\n",
       "      <td>1.240000e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11575</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.572500e+07</td>\n",
       "      <td>5.269000e+06</td>\n",
       "      <td>2.099400e+07</td>\n",
       "      <td>-2.099400e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.113800e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11576</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.755251e+06</td>\n",
       "      <td>3.755251e+06</td>\n",
       "      <td>-3.755251e+06</td>\n",
       "      <td>1.105849e+07</td>\n",
       "      <td>-1.482451e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11577</th>\n",
       "      <td>5.488438e+07</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>3.659379e+07</td>\n",
       "      <td>1.829059e+07</td>\n",
       "      <td>1.652633e+06</td>\n",
       "      <td>7.020320e+06</td>\n",
       "      <td>8.672953e+06</td>\n",
       "      <td>9.617636e+06</td>\n",
       "      <td>1.239170e+06</td>\n",
       "      <td>8.416324e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11578</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.031715e+07</td>\n",
       "      <td>4.521349e+06</td>\n",
       "      <td>1.664863e+07</td>\n",
       "      <td>-1.664863e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.664769e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11579</th>\n",
       "      <td>5.301900e+07</td>\n",
       "      <td>0.0243</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.301900e+07</td>\n",
       "      <td>5.668400e+07</td>\n",
       "      <td>2.945700e+07</td>\n",
       "      <td>8.614600e+07</td>\n",
       "      <td>-3.312700e+07</td>\n",
       "      <td>1.660000e+05</td>\n",
       "      <td>-3.438500e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11579 rows Ã— 231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Revenue  Revenue Growth  Cost of Revenue  Gross Profit  \\\n",
       "1      3.734148e+09          1.1737     2.805625e+09  9.285226e+08   \n",
       "2      1.790960e+10          0.0076     1.153980e+10  6.369800e+09   \n",
       "3      5.727000e+09          0.0214     3.523600e+09  2.203400e+09   \n",
       "4      1.972400e+10          0.0083     1.304100e+10  6.683000e+09   \n",
       "5      8.268000e+09          0.0268     5.297000e+09  2.971000e+09   \n",
       "...             ...             ...              ...           ...   \n",
       "11575  0.000000e+00          0.0000     0.000000e+00  0.000000e+00   \n",
       "11576  0.000000e+00          0.0000     0.000000e+00  0.000000e+00   \n",
       "11577  5.488438e+07          0.2210     3.659379e+07  1.829059e+07   \n",
       "11578  0.000000e+00          0.0000     0.000000e+00  0.000000e+00   \n",
       "11579  5.301900e+07          0.0243     0.000000e+00  5.301900e+07   \n",
       "\n",
       "       R&D Expenses  SG&A Expense  Operating Expenses  Operating Income  \\\n",
       "1      1.083303e+08  3.441414e+08        7.939267e+08      1.345959e+08   \n",
       "2      0.000000e+00  3.474300e+09        3.412400e+09      2.957400e+09   \n",
       "3      0.000000e+00  1.480500e+09        1.598700e+09      6.047000e+08   \n",
       "4      0.000000e+00  3.709000e+09        4.162000e+09      2.521000e+09   \n",
       "5      1.220000e+08  1.505000e+09        1.704000e+09      1.267000e+09   \n",
       "...             ...           ...                 ...               ...   \n",
       "11575  1.572500e+07  5.269000e+06        2.099400e+07     -2.099400e+07   \n",
       "11576  0.000000e+00  3.755251e+06        3.755251e+06     -3.755251e+06   \n",
       "11577  1.652633e+06  7.020320e+06        8.672953e+06      9.617636e+06   \n",
       "11578  1.031715e+07  4.521349e+06        1.664863e+07     -1.664863e+07   \n",
       "11579  5.668400e+07  2.945700e+07        8.614600e+07     -3.312700e+07   \n",
       "\n",
       "       Interest Expense  Earnings before Tax  ...  Consumer Cyclical  \\\n",
       "1          1.214869e+07         1.753823e+08  ...                  0   \n",
       "2          3.024000e+08         2.707700e+09  ...                  0   \n",
       "3          6.040000e+07         4.669000e+08  ...                  0   \n",
       "4          2.840000e+08         2.382000e+09  ...                  0   \n",
       "5          1.220000e+08         1.240000e+09  ...                  0   \n",
       "...                 ...                  ...  ...                ...   \n",
       "11575      0.000000e+00        -2.113800e+07  ...                  0   \n",
       "11576      1.105849e+07        -1.482451e+07  ...                  0   \n",
       "11577      1.239170e+06         8.416324e+06  ...                  0   \n",
       "11578      0.000000e+00        -1.664769e+07  ...                  0   \n",
       "11579      1.660000e+05        -3.438500e+07  ...                  0   \n",
       "\n",
       "       Consumer Defensive  Energy  Financial Services  Healthcare  \\\n",
       "1                       1       0                   0           0   \n",
       "2                       1       0                   0           0   \n",
       "3                       1       0                   0           0   \n",
       "4                       1       0                   0           0   \n",
       "5                       1       0                   0           0   \n",
       "...                   ...     ...                 ...         ...   \n",
       "11575                   0       0                   0           1   \n",
       "11576                   0       0                   0           0   \n",
       "11577                   0       0                   0           0   \n",
       "11578                   0       0                   0           0   \n",
       "11579                   0       0                   0           1   \n",
       "\n",
       "       Industrials  Real Estate  Technology  Utilities  Signal  \n",
       "1                0            0           0          0    Sell  \n",
       "2                0            0           0          0     Buy  \n",
       "3                0            0           0          0     Buy  \n",
       "4                0            0           0          0     Buy  \n",
       "5                0            0           0          0     Buy  \n",
       "...            ...          ...         ...        ...     ...  \n",
       "11575            0            0           0          0     Buy  \n",
       "11576            0            1           0          0    Sell  \n",
       "11577            0            0           0          0    Sell  \n",
       "11578            1            0           0          0    Sell  \n",
       "11579            0            0           0          0     Buy  \n",
       "\n",
       "[11579 rows x 231 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arrange response variable with dummy variable\n",
    "X = df.drop('Signal', axis=1)\n",
    "y = pd.get_dummies(df['Signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Buy</th>\n",
       "      <th>Hold</th>\n",
       "      <th>Sell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Buy  Hold  Sell\n",
       "1    0     0     1\n",
       "2    1     0     0\n",
       "3    1     0     0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspecting if transition worked for response variable\n",
    "y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale our data to avoid issues (X_scaled = X_std * (max - min) + min)\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating an instance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#adjusting scaler only based on the training set (without test values to prevent data leakage)\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform the transformation\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8105, 230)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import functions to create a neural network model based on keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up a empty sequential model\n",
    "model = Sequential()\n",
    "\n",
    "#adding dense layers (=regular densely-connected NN layer)\n",
    "#rule of thumb: Create as many NN as features for first layer\n",
    "model.add(Dense(230, activation='relu'))\n",
    "\n",
    "#dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#repeat process, but reduce amount of neurons for each step by half\n",
    "model.add(Dense(115, activation='relu'))\n",
    "model.add(Dropout(0.2))        \n",
    "          \n",
    "model.add(Dense(57, activation='relu'))\n",
    "model.add(Dropout(0.2))           \n",
    "  \n",
    "#1 Output neuron per class\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "#optimize a multi-class classification problem\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "32/32 [==============================] - 3s 13ms/step - loss: 0.9327 - accuracy: 0.4893\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.8949 - accuracy: 0.5243\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.8799 - accuracy: 0.5162\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.8850 - accuracy: 0.5199\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.8888 - accuracy: 0.5176\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.8842 - accuracy: 0.5352\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8839 - accuracy: 0.5277\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8792 - accuracy: 0.5259\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8768 - accuracy: 0.5316\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8777 - accuracy: 0.5287\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.8747 - accuracy: 0.5326\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8719 - accuracy: 0.5386\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8829 - accuracy: 0.5365\n",
      "Epoch 14/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8697 - accuracy: 0.5446\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8843 - accuracy: 0.5240: 0s - loss: 0.8960 - accuracy\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8728 - accuracy: 0.5366\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8576 - accuracy: 0.5451\n",
      "Epoch 18/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8763 - accuracy: 0.5454\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8761 - accuracy: 0.5435\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8689 - accuracy: 0.5328\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.8737 - accuracy: 0.5463\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.8705 - accuracy: 0.5472\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.8682 - accuracy: 0.5463\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8632 - accuracy: 0.5475\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8674 - accuracy: 0.5543\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.8640 - accuracy: 0.5469\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8604 - accuracy: 0.5491\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8616 - accuracy: 0.5486\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8651 - accuracy: 0.5441\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.8632 - accuracy: 0.5475\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.8628 - accuracy: 0.5463\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.8607 - accuracy: 0.5619\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8691 - accuracy: 0.5476\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8665 - accuracy: 0.5495\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8663 - accuracy: 0.5543\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8533 - accuracy: 0.5582\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8542 - accuracy: 0.5629\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8543 - accuracy: 0.5349 0s - loss: 0.8489 - accuracy\n",
      "Epoch 39/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.8559 - accuracy: 0.5555 0s - loss: 0.8529 - accu\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8566 - accuracy: 0.5524\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.8584 - accuracy: 0.5528\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8558 - accuracy: 0.5537\n",
      "Epoch 43/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8519 - accuracy: 0.5646\n",
      "Epoch 44/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.8581 - accuracy: 0.5518\n",
      "Epoch 45/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8576 - accuracy: 0.5493\n",
      "Epoch 46/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.8530 - accuracy: 0.5542\n",
      "Epoch 47/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.8607 - accuracy: 0.5506\n",
      "Epoch 48/300\n",
      "32/32 [==============================] - 0s 16ms/step - loss: 0.8583 - accuracy: 0.5599\n",
      "Epoch 49/300\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.8623 - accuracy: 0.5614\n",
      "Epoch 50/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.8578 - accuracy: 0.5519\n",
      "Epoch 51/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8644 - accuracy: 0.5506\n",
      "Epoch 52/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8666 - accuracy: 0.5552\n",
      "Epoch 53/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8554 - accuracy: 0.5639\n",
      "Epoch 54/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.8529 - accuracy: 0.5658\n",
      "Epoch 55/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8502 - accuracy: 0.5598\n",
      "Epoch 56/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8613 - accuracy: 0.5658\n",
      "Epoch 57/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.8660 - accuracy: 0.5628\n",
      "Epoch 58/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8410 - accuracy: 0.5669\n",
      "Epoch 59/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8564 - accuracy: 0.5562\n",
      "Epoch 60/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8439 - accuracy: 0.5600\n",
      "Epoch 61/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8567 - accuracy: 0.5624\n",
      "Epoch 62/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8579 - accuracy: 0.5607\n",
      "Epoch 63/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8540 - accuracy: 0.5512\n",
      "Epoch 64/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8478 - accuracy: 0.5642\n",
      "Epoch 65/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8610 - accuracy: 0.5617\n",
      "Epoch 66/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8494 - accuracy: 0.5621\n",
      "Epoch 67/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8521 - accuracy: 0.5604\n",
      "Epoch 68/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8500 - accuracy: 0.5592\n",
      "Epoch 69/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8528 - accuracy: 0.5610\n",
      "Epoch 70/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8455 - accuracy: 0.5579\n",
      "Epoch 71/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8570 - accuracy: 0.5570\n",
      "Epoch 72/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8577 - accuracy: 0.5607\n",
      "Epoch 73/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8427 - accuracy: 0.5670\n",
      "Epoch 74/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8509 - accuracy: 0.5634\n",
      "Epoch 75/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8529 - accuracy: 0.5727\n",
      "Epoch 76/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8423 - accuracy: 0.5654\n",
      "Epoch 77/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8551 - accuracy: 0.5596\n",
      "Epoch 78/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8418 - accuracy: 0.5672\n",
      "Epoch 79/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8462 - accuracy: 0.5644\n",
      "Epoch 80/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8460 - accuracy: 0.5629\n",
      "Epoch 81/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8433 - accuracy: 0.5733\n",
      "Epoch 82/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8465 - accuracy: 0.5658\n",
      "Epoch 83/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8493 - accuracy: 0.5586\n",
      "Epoch 84/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8461 - accuracy: 0.5664\n",
      "Epoch 85/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8521 - accuracy: 0.5680\n",
      "Epoch 86/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.8425 - accuracy: 0.5707 0s - loss: 0.8476 - accu\n",
      "Epoch 87/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.8535 - accuracy: 0.5649\n",
      "Epoch 88/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.8434 - accuracy: 0.5713\n",
      "Epoch 89/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8411 - accuracy: 0.5700 0s - loss: 0.8354 - accura - ETA: 0s - loss: 0.8398 - accuracy: \n",
      "Epoch 90/300\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.8342 - accuracy: 0.5710 0s - loss: 0.8059 - accuracy\n",
      "Epoch 91/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.8415 - accuracy: 0.5701\n",
      "Epoch 92/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8427 - accuracy: 0.5673\n",
      "Epoch 93/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8545 - accuracy: 0.5622\n",
      "Epoch 94/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8474 - accuracy: 0.5609\n",
      "Epoch 95/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8324 - accuracy: 0.5739\n",
      "Epoch 96/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8336 - accuracy: 0.5644\n",
      "Epoch 97/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8500 - accuracy: 0.5764\n",
      "Epoch 98/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8393 - accuracy: 0.5745\n",
      "Epoch 99/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8518 - accuracy: 0.5694\n",
      "Epoch 100/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8370 - accuracy: 0.5718\n",
      "Epoch 101/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8336 - accuracy: 0.5711\n",
      "Epoch 102/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8410 - accuracy: 0.5725\n",
      "Epoch 103/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8402 - accuracy: 0.5574\n",
      "Epoch 104/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8422 - accuracy: 0.5701\n",
      "Epoch 105/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8351 - accuracy: 0.5712\n",
      "Epoch 106/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8293 - accuracy: 0.5810\n",
      "Epoch 107/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8357 - accuracy: 0.5650\n",
      "Epoch 108/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8416 - accuracy: 0.5742\n",
      "Epoch 109/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8346 - accuracy: 0.5796\n",
      "Epoch 110/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8569 - accuracy: 0.5633\n",
      "Epoch 111/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8410 - accuracy: 0.5617\n",
      "Epoch 112/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8402 - accuracy: 0.5741\n",
      "Epoch 113/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8202 - accuracy: 0.5939\n",
      "Epoch 114/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8377 - accuracy: 0.5727\n",
      "Epoch 115/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8415 - accuracy: 0.5682\n",
      "Epoch 116/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8301 - accuracy: 0.5778\n",
      "Epoch 117/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.8368 - accuracy: 0.5803\n",
      "Epoch 118/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.8327 - accuracy: 0.5690\n",
      "Epoch 119/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.8353 - accuracy: 0.5702\n",
      "Epoch 120/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8328 - accuracy: 0.5795 0s - loss: 0.8334 - accuracy\n",
      "Epoch 121/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8267 - accuracy: 0.5794\n",
      "Epoch 122/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.8348 - accuracy: 0.5749\n",
      "Epoch 123/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8290 - accuracy: 0.5776\n",
      "Epoch 124/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8337 - accuracy: 0.5767\n",
      "Epoch 125/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8276 - accuracy: 0.5764\n",
      "Epoch 126/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8393 - accuracy: 0.5739\n",
      "Epoch 127/300\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 0.8267 - accuracy: 0.5747\n",
      "Epoch 128/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8189 - accuracy: 0.5804\n",
      "Epoch 129/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8258 - accuracy: 0.5805\n",
      "Epoch 130/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.8273 - accuracy: 0.5845\n",
      "Epoch 131/300\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.8160 - accuracy: 0.5731\n",
      "Epoch 132/300\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.8200 - accuracy: 0.5774\n",
      "Epoch 133/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8294 - accuracy: 0.5768\n",
      "Epoch 134/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8250 - accuracy: 0.5834\n",
      "Epoch 135/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8295 - accuracy: 0.5723\n",
      "Epoch 136/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8211 - accuracy: 0.5780\n",
      "Epoch 137/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8216 - accuracy: 0.5909\n",
      "Epoch 138/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8258 - accuracy: 0.5818\n",
      "Epoch 139/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8252 - accuracy: 0.5901\n",
      "Epoch 140/300\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.8318 - accuracy: 0.5823\n",
      "Epoch 141/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8288 - accuracy: 0.5836\n",
      "Epoch 142/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8171 - accuracy: 0.5926\n",
      "Epoch 143/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8234 - accuracy: 0.5830\n",
      "Epoch 144/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8237 - accuracy: 0.5826\n",
      "Epoch 145/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.8222 - accuracy: 0.58 - 0s 11ms/step - loss: 0.8222 - accuracy: 0.5895\n",
      "Epoch 146/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8215 - accuracy: 0.5883\n",
      "Epoch 147/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8185 - accuracy: 0.5853\n",
      "Epoch 148/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8168 - accuracy: 0.5834\n",
      "Epoch 149/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8258 - accuracy: 0.5814\n",
      "Epoch 150/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.8240 - accuracy: 0.5799\n",
      "Epoch 151/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8046 - accuracy: 0.6013\n",
      "Epoch 152/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8111 - accuracy: 0.5930\n",
      "Epoch 153/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8224 - accuracy: 0.5799\n",
      "Epoch 154/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.8280 - accuracy: 0.5893\n",
      "Epoch 155/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8218 - accuracy: 0.5797\n",
      "Epoch 156/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8138 - accuracy: 0.5912\n",
      "Epoch 157/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8248 - accuracy: 0.5840\n",
      "Epoch 158/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8091 - accuracy: 0.5932\n",
      "Epoch 159/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8146 - accuracy: 0.5948\n",
      "Epoch 160/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8195 - accuracy: 0.5844\n",
      "Epoch 161/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8221 - accuracy: 0.5796\n",
      "Epoch 162/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8151 - accuracy: 0.5866\n",
      "Epoch 163/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8114 - accuracy: 0.5874\n",
      "Epoch 164/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8009 - accuracy: 0.5993\n",
      "Epoch 165/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8279 - accuracy: 0.5745\n",
      "Epoch 166/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8101 - accuracy: 0.5932\n",
      "Epoch 167/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8156 - accuracy: 0.5949\n",
      "Epoch 168/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.8065 - accuracy: 0.5919\n",
      "Epoch 169/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.8069 - accuracy: 0.5949\n",
      "Epoch 170/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.8156 - accuracy: 0.5831\n",
      "Epoch 171/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8188 - accuracy: 0.5895\n",
      "Epoch 172/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8105 - accuracy: 0.5917\n",
      "Epoch 173/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8104 - accuracy: 0.5779\n",
      "Epoch 174/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.8186 - accuracy: 0.5851\n",
      "Epoch 175/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8137 - accuracy: 0.5924\n",
      "Epoch 176/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8155 - accuracy: 0.5899\n",
      "Epoch 177/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8193 - accuracy: 0.5842\n",
      "Epoch 178/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8136 - accuracy: 0.5925\n",
      "Epoch 179/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.8018 - accuracy: 0.5903\n",
      "Epoch 180/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8142 - accuracy: 0.5873\n",
      "Epoch 181/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8041 - accuracy: 0.5946\n",
      "Epoch 182/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.8048 - accuracy: 0.6008\n",
      "Epoch 183/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.8097 - accuracy: 0.5908 0s - loss: 0.8097 - accuracy: 0.59\n",
      "Epoch 184/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.8065 - accuracy: 0.5975\n",
      "Epoch 185/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8112 - accuracy: 0.5850\n",
      "Epoch 186/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.8136 - accuracy: 0.5859\n",
      "Epoch 187/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.8082 - accuracy: 0.5988\n",
      "Epoch 188/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.7988 - accuracy: 0.5979\n",
      "Epoch 189/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8173 - accuracy: 0.5900\n",
      "Epoch 190/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.8134 - accuracy: 0.5891\n",
      "Epoch 191/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.8026 - accuracy: 0.6020\n",
      "Epoch 192/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8115 - accuracy: 0.5954\n",
      "Epoch 193/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.7966 - accuracy: 0.5994\n",
      "Epoch 194/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8173 - accuracy: 0.5817\n",
      "Epoch 195/300\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.8023 - accuracy: 0.5956\n",
      "Epoch 196/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8036 - accuracy: 0.5987\n",
      "Epoch 197/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8107 - accuracy: 0.5951\n",
      "Epoch 198/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8149 - accuracy: 0.5936\n",
      "Epoch 199/300\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.8006 - accuracy: 0.59 - 0s 12ms/step - loss: 0.8008 - accuracy: 0.5967\n",
      "Epoch 200/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8018 - accuracy: 0.6010\n",
      "Epoch 201/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8106 - accuracy: 0.5925\n",
      "Epoch 202/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8112 - accuracy: 0.5977\n",
      "Epoch 203/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7946 - accuracy: 0.5985\n",
      "Epoch 204/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8112 - accuracy: 0.5898\n",
      "Epoch 205/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8082 - accuracy: 0.5941\n",
      "Epoch 206/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8084 - accuracy: 0.5912\n",
      "Epoch 207/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8167 - accuracy: 0.5979\n",
      "Epoch 208/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7967 - accuracy: 0.5978\n",
      "Epoch 209/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8015 - accuracy: 0.5977\n",
      "Epoch 210/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8066 - accuracy: 0.6011\n",
      "Epoch 211/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7970 - accuracy: 0.6043\n",
      "Epoch 212/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8031 - accuracy: 0.5936\n",
      "Epoch 213/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8015 - accuracy: 0.5958\n",
      "Epoch 214/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8091 - accuracy: 0.5984 0s - loss: 0.8395 - accu\n",
      "Epoch 215/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8043 - accuracy: 0.5971\n",
      "Epoch 216/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7850 - accuracy: 0.6092\n",
      "Epoch 217/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7947 - accuracy: 0.6034\n",
      "Epoch 218/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.8036 - accuracy: 0.5993\n",
      "Epoch 219/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7958 - accuracy: 0.6013\n",
      "Epoch 220/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7971 - accuracy: 0.5978\n",
      "Epoch 221/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7947 - accuracy: 0.6048\n",
      "Epoch 222/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7923 - accuracy: 0.6056\n",
      "Epoch 223/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7920 - accuracy: 0.6060 0s - loss: 0.7906 - accuracy: 0.\n",
      "Epoch 224/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7992 - accuracy: 0.6018\n",
      "Epoch 225/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7975 - accuracy: 0.6025\n",
      "Epoch 226/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7970 - accuracy: 0.6096\n",
      "Epoch 227/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.8020 - accuracy: 0.5953\n",
      "Epoch 228/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7993 - accuracy: 0.6007\n",
      "Epoch 229/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7877 - accuracy: 0.6059\n",
      "Epoch 230/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.7965 - accuracy: 0.6098\n",
      "Epoch 231/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.8062 - accuracy: 0.5864\n",
      "Epoch 232/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.7871 - accuracy: 0.6021\n",
      "Epoch 233/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7973 - accuracy: 0.6041\n",
      "Epoch 234/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.7930 - accuracy: 0.6048\n",
      "Epoch 235/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.7809 - accuracy: 0.6123\n",
      "Epoch 236/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7863 - accuracy: 0.6085\n",
      "Epoch 237/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.7979 - accuracy: 0.6007\n",
      "Epoch 238/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8050 - accuracy: 0.5956\n",
      "Epoch 239/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 8ms/step - loss: 0.7812 - accuracy: 0.5989\n",
      "Epoch 240/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7939 - accuracy: 0.5958\n",
      "Epoch 241/300\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.7899 - accuracy: 0.6058\n",
      "Epoch 242/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.7964 - accuracy: 0.5997\n",
      "Epoch 243/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.7941 - accuracy: 0.5974\n",
      "Epoch 244/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.7950 - accuracy: 0.6056\n",
      "Epoch 245/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.7799 - accuracy: 0.6179\n",
      "Epoch 246/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.7834 - accuracy: 0.6106\n",
      "Epoch 247/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.8017 - accuracy: 0.5996 0s - loss: 0.8070 - accuracy\n",
      "Epoch 248/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7798 - accuracy: 0.6018\n",
      "Epoch 249/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.8016 - accuracy: 0.5913\n",
      "Epoch 250/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7887 - accuracy: 0.6094\n",
      "Epoch 251/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7995 - accuracy: 0.5982 0s - loss: 0.8024 - accuracy\n",
      "Epoch 252/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7830 - accuracy: 0.6080\n",
      "Epoch 253/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7842 - accuracy: 0.6105\n",
      "Epoch 254/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7886 - accuracy: 0.6069\n",
      "Epoch 255/300\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.7844 - accuracy: 0.6088\n",
      "Epoch 256/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7914 - accuracy: 0.6016\n",
      "Epoch 257/300\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.7975 - accuracy: 0.5988\n",
      "Epoch 258/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7874 - accuracy: 0.5979\n",
      "Epoch 259/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7748 - accuracy: 0.6050\n",
      "Epoch 260/300\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.7806 - accuracy: 0.6098\n",
      "Epoch 261/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7859 - accuracy: 0.6072\n",
      "Epoch 262/300\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.7836 - accuracy: 0.6025\n",
      "Epoch 263/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7748 - accuracy: 0.6201\n",
      "Epoch 264/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.7871 - accuracy: 0.6117\n",
      "Epoch 265/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.7932 - accuracy: 0.5921\n",
      "Epoch 266/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7878 - accuracy: 0.6108\n",
      "Epoch 267/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7837 - accuracy: 0.6141\n",
      "Epoch 268/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7728 - accuracy: 0.6063\n",
      "Epoch 269/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7746 - accuracy: 0.6113\n",
      "Epoch 270/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7739 - accuracy: 0.6140\n",
      "Epoch 271/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.7792 - accuracy: 0.6170\n",
      "Epoch 272/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.7846 - accuracy: 0.6172\n",
      "Epoch 273/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.7835 - accuracy: 0.6059\n",
      "Epoch 274/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.7802 - accuracy: 0.6090\n",
      "Epoch 275/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.7761 - accuracy: 0.6143\n",
      "Epoch 276/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.7765 - accuracy: 0.6141\n",
      "Epoch 277/300\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.7703 - accuracy: 0.6135\n",
      "Epoch 278/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7688 - accuracy: 0.6256 0s - loss: 0.7565 \n",
      "Epoch 279/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.7801 - accuracy: 0.6070\n",
      "Epoch 280/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7669 - accuracy: 0.6064\n",
      "Epoch 281/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.7744 - accuracy: 0.6121\n",
      "Epoch 282/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7828 - accuracy: 0.5980\n",
      "Epoch 283/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7787 - accuracy: 0.6126 0s - loss: 0.7721 - ac\n",
      "Epoch 284/300\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 0.7783 - accuracy: 0.6229\n",
      "Epoch 285/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7783 - accuracy: 0.6201\n",
      "Epoch 286/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7830 - accuracy: 0.6077\n",
      "Epoch 287/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7732 - accuracy: 0.6063\n",
      "Epoch 288/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7928 - accuracy: 0.6008 0s - loss: 0.8091 - accuracy: 0. - ETA: 0s - loss: 0.8003 - accura\n",
      "Epoch 289/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.7750 - accuracy: 0.6105\n",
      "Epoch 290/300\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.7725 - accuracy: 0.6219\n",
      "Epoch 291/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.7798 - accuracy: 0.6070\n",
      "Epoch 292/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.7791 - accuracy: 0.6141\n",
      "Epoch 293/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7696 - accuracy: 0.6107\n",
      "Epoch 294/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7701 - accuracy: 0.6144\n",
      "Epoch 295/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7984 - accuracy: 0.5977\n",
      "Epoch 296/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7592 - accuracy: 0.6164\n",
      "Epoch 297/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7665 - accuracy: 0.6201\n",
      "Epoch 298/300\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.7712 - accuracy: 0.6220\n",
      "Epoch 299/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7748 - accuracy: 0.6139\n",
      "Epoch 300/300\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7770 - accuracy: 0.6087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19cdd99b580>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model to our training date, \n",
    "#Our model will go trough the training data 300 times\n",
    "model.fit(x=X_train, y=y_train, epochs=300, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x19ce30a5eb0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting training history of our algorithm\n",
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7477112412452698, 0.6261566877365112]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#analysing performance on training data\n",
    "model.evaluate(X_train,y_train,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.063571572303772, 0.5616004467010498]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#analysing performance on test data\n",
    "model.evaluate(X_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6158     Sell\n",
       "5623     Sell\n",
       "3789      Buy\n",
       "457      Sell\n",
       "161      Sell\n",
       "         ... \n",
       "148      Sell\n",
       "11277     Buy\n",
       "9952      Buy\n",
       "6840      Buy\n",
       "8468     Sell\n",
       "Length: 3474, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reverse dummy variables (https://stackoverflow.com/questions/50607740/reverse-a-get-dummies-encoding-in-pandas)\n",
    "y_true = y_test.idxmax(axis=1)\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to numbers for y_true\n",
    "SignalFac = []\n",
    "\n",
    "for string in y_true:\n",
    "    if string == 'Sell':\n",
    "        SignalFac.append(2)\n",
    "    elif string == 'Hold':\n",
    "        SignalFac.append(1)    \n",
    "    else:\n",
    "        SignalFac.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50      1404\n",
      "           1       0.09      0.01      0.02       224\n",
      "           2       0.61      0.68      0.64      1846\n",
      "\n",
      "    accuracy                           0.56      3474\n",
      "   macro avg       0.40      0.39      0.39      3474\n",
      "weighted avg       0.53      0.56      0.54      3474\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(SignalFac,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see F1 Score for predicting a Sell is the highest followed by the buy prediction\n",
    "#However, our model has some trouble to predict the hold signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sell    6153\n",
       "Buy     4681\n",
       "Hold     745\n",
       "Name: Signal, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Signal'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5313930391225494"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if our model learned something\n",
    "6153/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see the accuracy of our model is better than the default guess on sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 698   10  696]\n",
      " [ 110    2  112]\n",
      " [ 584   11 1251]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(SignalFac,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
